# Create pretraining_template column in ds
def apply_prompt_template(row, tokenizer, max_length=250):
    """
    Apply a prompt template to a row of the dataset.

    Args:
        row: Row of dataset
        tokenizer: Huggingface tokenizer
        max_length: Maximum length of source text in words.
    Returns:
        row: Row with prompt template applied in "text_prompt" column.
    """

    # Keep only first max_length words of text
    text_preview = " ".join(row["text"].split()[:max_length])
    document_context = "###\n" + text_preview + "\n###\n"
    prompt = """Does the previous paragraph demarcated within ### and ###
    contain informative signal for pre-training a large-language model in Swedish?
    An informative datapoint should be well-formatted, contain some
    usable knowledge of the world, and strictly NOT have any harmful,
    racist, sexist, autogenerated or autotranslated marketing, etc. content.

    OPTIONS:

    - yes
    - no
    """
    prompt = """Does the following paragraph demarcated within ### and ###
    contain informative signal for pre-training a large-language model in Swedish?
    An informative datapoint should be well-formatted, contain some
    usable knowledge of the world, and not have any autogenerated or autotranslated marketing, etc. content.

    OPTIONS:

    - yes
    - no


    """

    text = document_context + prompt
    text = prompt + document_context

    messages = [
        {
            "role": "user",
            "content": text,
        }
    ]

    text_prompt = tokenizer.apply_chat_template(messages, tokenize=False)
    row["text_prompt"] = text_prompt
    return row


def apply_fineweb_prompt(
    example, prompt, tokenizer, max_words=300, language="Swedish", prompt_type="llama"
):
    # format the extract with python .format and return the prompt as dict
    extract = example["text"]
    extract = " ".join(extract.split(" ")[0:max_words])

    prompt = prompt.format(extract=extract, language=language)

    if prompt_type == "gemma":
        messages = [
            {"role": "user", "content": prompt},
        ]
        example["text_prompt"] = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
    else:
        messages = [
            {
                "role": "system",
                "content": "You are a helpful AI assistant that can evaluate the educational value of a web page.",
            },
            {"role": "user", "content": prompt},
        ]
        example["text_prompt"] = tokenizer.apply_chat_template(messages, tokenize=False)

    return example
